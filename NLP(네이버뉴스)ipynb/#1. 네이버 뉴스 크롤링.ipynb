{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade2c7f6",
   "metadata": {},
   "source": [
    "# 요약\n",
    "\n",
    "- 개요 \n",
    "    - 주식의 가격에는 다양한 요소들이 영향을 미치지만 그 중 뉴스에 민감하다고 판단하여 뉴스 기사의 제목을 분석하여 긍정/부정 평가를 한다. \n",
    "\n",
    "\n",
    "1. 데이터 수집\n",
    "    - 최근 1일, 정확도 순서로 검색어 입력 시 그에 대한 뉴스 제목 정보를 크롤링한다.\n",
    "    \n",
    "2. 전처리\n",
    "    - bert 모델로 제목에 대해서 형태소 분석을 하고 tokenize를 진행한다.\n",
    "    - 제목을 수치화한 'sentiment' 값을 얻어내고 데이터는 아래와 같다.\n",
    "    - train_data\n",
    "       - feature : 미리 크롤링한 기사 제목에 대한 bert 모델을 적용한 수치값 (sentiment) (1과 0 사이의 유리수)\n",
    "       - target : 부정/긍정. 0,1 (0은 부정 ,1은 긍정) (label)\n",
    "           - train_data의 target은 직접 기사 제목을 읽고 라벨링을 수행함 \n",
    "           - 일단 180개 정도 수행. 정확도를 올리기 위해 추가적인 라벨링 수행할 예정\n",
    "    - test_data\n",
    "       - feature : 1일간 크롤링한 기사 제목에 대한 bert 모델을 적용한 수치값 (sentiment)\n",
    "       - target : 0,1 (label)\n",
    "           - test_data의 target은 feature를 기반해서 0.5 이상이면 긍정, 0.5 미만이면 부정으로 설정   \n",
    "        \n",
    "3. 모델링\n",
    "- bert 모델\n",
    "    - 문장의 형태소 분석과, sentiment 값을 추출하기 위해 사용\n",
    "- DecisionTree, RandomForestClassifier, LogisticRegression 모델 적용\n",
    "    \n",
    "4. 성능 평가\n",
    "- 성능 평가를 위해 DecisionTree, RandomForestClassifier, LogisticRegression 회귀를 적용하여 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8bf180",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 크롤링\n",
    "- 검색어 입력 받고 정확도순, 최근 1일 데이터 수집\n",
    "- 제목 수준에서 긍/부정 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0a912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째 테스트? 1\n",
      "검색할 키워드를 입력해주세요:삼성전자\n",
      "\n",
      "크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):1\n",
      "\n",
      "크롤링할 시작 페이지:  1 페이지\n",
      "\n",
      "크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):10\n",
      "\n",
      "크롤링할 종료 페이지:  10 페이지\n",
      "생성url:  ['https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=1', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=11', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=21', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=31', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=41', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=51', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=61', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=71', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=81', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&pd=4&start=91']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [00:07<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 기사 갯수: 총  100 개\n",
      "\n",
      "[뉴스 제목]\n",
      "[\"삼성전자, 갤럭시·노트북·TV서 '자가 수리' 국내 도입\", '삼성전자 갤럭시, 이제 내손으로 고친다', '[특징주] 삼성전자·SK하이닉스, 52주 신고가 기록', '\"청년에 일경험\"…SK하이닉스·삼성전자 등 2100명 모집', '작년 주주 배당액 1위는 삼성전자…총 배당금은 6.7%↓', '“고장난 갤럭시 직접 고친다”…삼성전자, ‘자가 수리’ 국내 도입', '\"이번엔 중동\"…삼성전자, 두바이 리조트서 2023년형 TV 공개', '삼성전자, 두바이서 2023년형 네오QLED 신제품 선봬', '[특징주] 날개 단 삼성전자‧SK하이닉스…또 52주 최고가 경신', '\"이제 집에서 고치세요\"...삼성전자, \\'자가 수리\\' 국내 도입', '\"머스크 불만 터트릴 지경인데\"…삼성전자·하이닉스 웃는 이유', '삼성전자 신고가 행진…\"8만전자 간다\"', '\"삼성전자, 책임경영 필요성↑…이재용 등기이사 복귀 가능성\"', \"삼성전자, 외국인 폭풍매수에 '7만전자 굳히기'\", '삼성전자, 7.1만원 진입…반도체株 훈풍 지속[특징주]', '\"삼성전자가 또?\"...신제품에 적용한 \\'첨단 기술\\'은?', '로이터 “삼성전자·LG전자, 베트남 정부에 최저한세 보상 요구”', '삼성전자 신고가 행진…코스피, 연중 최고치', '신고가 행진 삼성전자·SK하이닉스…\"반도체 질주는 계속된다\"', \"'반갑다, 삼성전자'…반도체주 랠리에 1년만에 7만2000원선 회복\", \"삼성전자 '보행 보조 로봇' 특허 또 추가…'봇핏' 출시 임박?\", \"[증시신상품] 한화투자증권, '한화 델타랩 삼성전자우' 출시\", '삼성전자·SK하이닉스 연고점…반도체주 랠리 이어질까', \"삼성전자 vs TSMC…'AI 붐' 반도체 수혜주는 어디?\", \"코스피, 1년 만에 2580선…'파죽지세' 삼성전자 7.2만대\", '[STOCK] \"땡큐 엔비디아\" 삼성전자·SK하이닉스, 나란히 신고가', \"삼성전자 '8만전자'가 코앞?...52주 신고가 경신\", '삼성전자·SK하이닉스 또 희소식…엔비디아 재고 4년새 감소 [김익환의 컴퍼니워치]', '삼성전자, 52주 신고가 경신…반도체주 상승에 7만원대 안착[특징주]', '[특징주] 삼성전자·SK하이닉스…장초반 급등세', '삼성전자, 2023년형 TV 신제품으로 중동 소비자 공략', '삼성전자, 2023년형 TV 신제품으로 중동 소비자 공략', '삼성전자, 2023년형 TV 신제품으로 중동 소비자 공략', '\"AI 반도체 수요 늘어난다\"…삼성전자·SK하이닉스 신고가 경신', '삼성전자·SK하이닉스, 미국발 훈풍에 나란히 신고가', '삼성전자, 프리미엄TV로 중동 공략', '삼성전자, TV 신제품으로 중동 소비자 눈길 사로잡는다', '코스피·코스닥 상승 출발…삼성전자 1.85% ↑', \"삼성전자·SK하이닉스 '훈풍'…日 증시 주춤\", '삼성전자·SK하이닉스, ‘글로벌 AI 반도체 90% 점유’ 덕에 주가 ‘쭉쭉’…다음 체크포인트는? [투자360]', '제일건설㈜, 삼성전자 평택캠퍼스 직주근접 `지제역 반도체밸리 제일풍경채 2BL` 분양예정']\n",
      "\n",
      "[뉴스 링크]\n",
      "['https://n.news.naver.com/mnews/article/001/0013970361?sid=101', 'https://n.news.naver.com/mnews/article/021/0002574409?sid=101', 'https://n.news.naver.com/mnews/article/366/0000905345?sid=101', 'https://n.news.naver.com/mnews/article/003/0011886372?sid=102', 'https://n.news.naver.com/mnews/article/374/0000337683?sid=101', 'https://n.news.naver.com/mnews/article/030/0003103282?sid=105', 'https://n.news.naver.com/mnews/article/421/0006835624?sid=101', 'https://n.news.naver.com/mnews/article/008/0004893155?sid=101', 'https://n.news.naver.com/mnews/article/629/0000219308?sid=101', 'https://n.news.naver.com/mnews/article/057/0001745696?sid=101', 'https://n.news.naver.com/mnews/article/015/0004850447?sid=101', 'https://n.news.naver.com/mnews/article/011/0004196309?sid=101', 'https://n.news.naver.com/mnews/article/001/0013970411?sid=101', 'https://n.news.naver.com/mnews/article/018/0005496985?sid=101', 'https://n.news.naver.com/mnews/article/018/0005496476?sid=101', 'https://n.news.naver.com/mnews/article/057/0001745639?sid=101', 'https://n.news.naver.com/mnews/article/366/0000905492?sid=105', 'https://n.news.naver.com/mnews/article/032/0003226864?sid=101', 'https://n.news.naver.com/mnews/article/008/0004893227?sid=101', 'https://n.news.naver.com/mnews/article/374/0000337769?sid=101', 'https://n.news.naver.com/mnews/article/421/0006836192?sid=101', 'https://n.news.naver.com/mnews/article/001/0013970647?sid=101', 'https://n.news.naver.com/mnews/article/374/0000337656?sid=101', 'https://n.news.naver.com/mnews/article/015/0004850126?sid=104', 'https://n.news.naver.com/mnews/article/015/0004850357?sid=101', 'https://n.news.naver.com/mnews/article/417/0000924425?sid=101', 'https://n.news.naver.com/mnews/article/057/0001745692?sid=101', 'https://n.news.naver.com/mnews/article/015/0004850283?sid=101', 'https://n.news.naver.com/mnews/article/421/0006835640?sid=101', 'https://n.news.naver.com/mnews/article/277/0005265697?sid=101', 'https://n.news.naver.com/mnews/article/001/0013970341?sid=101', 'https://n.news.naver.com/mnews/article/001/0013970339?sid=101', 'https://n.news.naver.com/mnews/article/001/0013970343?sid=101', 'https://n.news.naver.com/mnews/article/015/0004850149?sid=101', 'https://n.news.naver.com/mnews/article/025/0003283504?sid=101', 'https://n.news.naver.com/mnews/article/021/0002574487?sid=101', 'https://n.news.naver.com/mnews/article/421/0006835610?sid=105', 'https://n.news.naver.com/mnews/article/277/0005265676?sid=101', 'https://n.news.naver.com/mnews/article/374/0000337688?sid=101', 'https://n.news.naver.com/mnews/article/016/0002150015?sid=101', 'https://n.news.naver.com/mnews/article/029/0002803640?sid=101']\n",
      "news_title:  41\n",
      "news_url:  41\n",
      "news_dates:  41\n",
      "중복 제거 후 행 개수:  41\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# 크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 페이지 입력 (1 페이지당 기사 10개 이하)\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num + 1\n",
    "    else:\n",
    "        return num + 9 * (num - 1)\n",
    "\n",
    "\n",
    "# search : 검색어, pd=4 : 최근 1일, start_page : 몇 페이지\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        # 정확도순(디폴트)으로 1일간의 뉴스(pd=4) \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&pd=4\"+\"&start=\" + str(\n",
    "            start_page)\n",
    "        return url\n",
    "    else:\n",
    "        # url 부분에서 정확도순서로 1일 데이터를 분류 가능\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&pd=4\"+\"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        return urls\n",
    "\n",
    "    # html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "\n",
    "# 기사 내용 크롤링 함수\n",
    "def news_attrs_crawler(articles, attrs):\n",
    "    attrs_content = []\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "# html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    # html 불러오기\n",
    "    original_html = requests.get(i, headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\n",
    "        \"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver, 'href')\n",
    "    return url\n",
    "\n",
    "\n",
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "# 테스트 횟수\n",
    "num = int(input('테스트 횟수 입력 : '))\n",
    "# 검색어 입력\n",
    "search = input(\"검색 키워드 입력 : \")\n",
    "# 검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\"))  # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 시작 페이지: \", page, \"페이지\")\n",
    "# 검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\"))  # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 종료 페이지: \", page2, \"페이지\")\n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search, page, page2)\n",
    "\n",
    "# 뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url = []\n",
    "\n",
    "# 일단 제목 수준으로 진행 (기사 내용은 추후에 적용 여부 판단)\n",
    "# news_contents = []\n",
    "\n",
    "news_dates = []\n",
    "for i in url:\n",
    "    url = articles_crawler(url)\n",
    "    news_url.append(url)\n",
    "\n",
    "\n",
    "# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "\n",
    "# 제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "# 1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1, news_url)\n",
    "\n",
    "# NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 뉴스 내용 크롤링\n",
    "for i in tqdm(final_urls):\n",
    "    # 각 기사 html get하기\n",
    "    news = requests.get(i, headers=headers)\n",
    "    news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "    # 뉴스 본문 가져오기 (일단 구현은 해놓음 but 일단 기사 제목 수준에서 진행)\n",
    "#     content = news_html.select(\"div#dic_area\")\n",
    "#     if content == []:\n",
    "#         content = news_html.select(\"#articeBody\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    #content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "#     content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "#     pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "#     content = content.replace(pattern2, '')\n",
    "    \n",
    "    news_titles.append(title)\n",
    "#    news_contents.append(content)\n",
    "\n",
    "    try:\n",
    "        html_date = news_html.select_one(\n",
    "            \"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1, repl='', string=str(news_date))\n",
    "    # 날짜 가져오기\n",
    "    news_dates.append(news_date)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \", (page2 + 1 - page) * 10, '개')\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(final_urls)\n",
    "#print(\"\\n[뉴스 내용]\")\n",
    "#print(news_contents)\n",
    "\n",
    "print('news_title: ', len(news_titles))\n",
    "print('news_url: ', len(final_urls))\n",
    "#print('news_contents: ', len(news_contents))\n",
    "print('news_dates: ', len(news_dates))\n",
    "\n",
    "###데이터 프레임으로 만들기###\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date': news_dates, 'title': news_titles})\n",
    "# news_df = pd.DataFrame({'date': news_dates, 'title': news_titles, 'link': final_urls, 'content': news_contents})\n",
    "\n",
    "# 중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first', ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \", len(news_df))\n",
    "\n",
    "# 데이터 프레임 저장\n",
    "now = datetime.datetime.now()\n",
    "news_df.to_csv(f'csv/{search} 뉴스 제목{num}.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(type(news_df['title']))\n",
    "# print(type(news_df['content']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a8de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a22f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CapstonStockProject",
   "language": "python",
   "name": "capstonestockproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
